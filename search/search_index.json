{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydantic Prompter","text":"<p>Pydantic Prompter is a lightweight tool designed for effortlessly constructing prompts and obtaining Pydantic objects as outputs.</p> <p>Seamlessly call LLMs like functions in Python with Pydantic Prompter.  It handles prompt creation and output parsing to custom models for providers like Cohere,  Bedrock, and OpenAI. Get OpenAi function calling API capabilities for any LLM.  Structured text generation with less code.</p> <p>The design of the library's API draws inspiration by DeclarAI. Other alternatives Outlines and Jsonformer</p>"},{"location":"#why-should-you-use-pydantic-prompter","title":"Why should you use Pydantic Prompter","text":"<p>\ud83d\udcbb Seamless LLM Integration: Pydantic Prompter supported multiple LLM providers, including Cohere, Bedrock, and OpenAI, right out of the box. This meant we could easily switch between providers without modifying our code, ensuring flexibility and portability.</p> <p>\ud83d\udce6 Structured Outputs: By leveraging Pydantic models, Pydantic Prompter automatically parsed the LLM's output into structured Python objects. Manual parsing became a thing of the past, and we enjoyed consistently formatted data that was a breeze to work with.</p> <p>\u270d\ufe0f Easy Prompt Engineering: Crafting effective prompts is an art, and Pydantic Prompter made us all masters. By defining prompts using Python classes and string interpolation, we created readable, maintainable, and reusable prompts.</p> <p>\ud83d\udd27 Reusable Components: Pydantic Prompter encouraged a modular approach, allowing us to define reusable prompt components such as instructions, examples, and constraints. This promoted code reuse and made maintaining our code effortless.</p> <p>\ud83d\udc1b Logging and Debugging: Built-in logging and debugging features meant we could quickly identify and resolve any issues, ensuring a smooth and efficient development process, free of bugs and errors.</p>"},{"location":"#install","title":"Install","text":""},{"location":"#openai","title":"OpenAI","text":"<pre><code>pip install 'pydantic-prompter[openai]'\n</code></pre>"},{"location":"#bedrock","title":"Bedrock","text":"<pre><code>pip install 'pydantic-prompter[bedrock]'\n</code></pre>"},{"location":"#cohere","title":"Cohere","text":"<pre><code>pip install 'pydantic-prompter[cohere]'\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#basic-usage","title":"Basic usage","text":"<p>To utilize Pydantic Prompter with Jinja2 templates, follow the example below:</p> <pre><code>from pydantic_prompter import Prompter\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-....\"\n\n\nclass RecommendedEntry(BaseModel):\n    id: str\n    name: str\n    reason: str = Field(description=\"Why this entry fits the query\", default=\"\")\n\n\nclass RecommendationResults(BaseModel):\n    title: str\n    entries: List[RecommendedEntry]\n\n\n@Prompter(llm=\"openai\", jinja=True, model_name=\"gpt-3.5-turbo-16k\")\ndef rank_recommendation(entries, query) -&gt; RecommendationResults:\n    \"\"\"\n    - system: You are a movie ranking expert\n    - user: |\n        Which of the following JSON entries fit best to the query.\n        order by best fit descending\n        Base your answer ONLY on the given JSON entries\n\n    - user: &gt;\n        The JSON entries:\n        {{ entries }}\n\n    - user: \"query: {{ query }}\"\n\n    \"\"\"\n\n\nmy_entries = (\n    '[{\"text\": \"Description: Four everyday suburban guys come together as a ....'\n)\nprint(rank_recommendation(entries=my_entries, query=\"Romantic comedy\"))\n\n# &gt;&gt;&gt; title='Romantic Comedy' entries=[RecommendedEntry(id='2312973', \\\n#       name='The Ugly Truth', reason='Romantic comedy genre')]\n\n\n# ==Debug you query==\nprint(rank_recommendation.build_string(entries=my_entries, query=\"Romantic comedy\"))\n\n# &gt;&gt;&gt; system: You are a movie ranking expert\n#     user: Which of the following JSON entries fit best to the query.\n#     order by best fit descending\n#     Base your answer ONLY on the given JSON entries\n#     user: The JSON entries: [{\"text\": \"Description: Four everyday suburban guys come together as a ....\n#     user: query: Romantic comedy\n</code></pre>"},{"location":"#simple-string-formatting","title":"Simple string formatting","text":"<p>For injecting conversation history through straightforward string formatting, refer to this example:</p> <pre><code>from pydantic import BaseModel\n\nfrom pydantic_prompter import Prompter\n\n\nclass QueryGPTResponse(BaseModel):\n    google_like_search_term: str\n\n\n@Prompter(llm=\"openai\", model_name=\"gpt-3.5-turbo\")\ndef search_query(history) -&gt; QueryGPTResponse:\n    \"\"\"\n    {history}\n\n    - user: |\n        Generate a Google-like search query text\n        encompassing all previous chat questions and answers\n    \"\"\"\n\n\nhistory = \"\"\"\n- user: Hi\n- assistant: what genre do you want to watch?\n- user: Comedy\n- assistant: do you want a movie or series?\n- user: Movie\n\"\"\"\nres = search_query.build_string(history=history)\nprint(res)\n\n# &gt;&gt;&gt; assistant: what genre do you want to watch?\n#     user: Comedy\n#     assistant: do you want a movie or series?\n#     user: Movie\n#     user: Generate a Google-like search query text\n#     encompassing all previous chat questions and answers\n</code></pre>"},{"location":"#jinja2-advance-usage","title":"Jinja2 advance usage","text":"<p>For more advanced usage involving Jinja2 loops to inject conversation history, consider the following code snippet:</p> <pre><code>from pydantic import BaseModel\n\nfrom pydantic_prompter import Prompter\n\n\nclass QueryGPTResponse(BaseModel):\n    google_like_search_term: str\n\n\n@Prompter(llm=\"openai\", jinja=True, model_name=\"gpt-3.5-turbo\")\ndef search_query(history) -&gt; QueryGPTResponse:\n    \"\"\"\n    {%- for line in history %}\n     {{ line }}\n    {% endfor %}\n\n    - user: |\n        Generate a Google-like search query text\n        encompassing all previous chat questions and answers\n    \"\"\"\n\n\nhistory = [\n    \"- user: Hi\"\n    \"- assistant: what genre do you want to watch?\",\n    \"- user: Comedy\",\n    \"- assistant: do you want a movie or series?\",\n    \"- user: Movie\",\n]\nres = search_query.build_string(history=history)\nprint(res)\n\n# &gt;&gt;&gt; assistant: what genre do you want to watch?\n#     user: Comedy\n#     assistant: do you want a movie or series?\n#     user: Movie\n#     user: Generate a Google-like search query text\n#     encompassing all previous chat questions and answers\n</code></pre>"},{"location":"#simple-typings","title":"Simple typings","text":"<p>Use <code>int</code>, <code>float</code>, <code>bool</code> or <code>str</code> <pre><code>from pydantic_prompter import Prompter\nimport os\n\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"...\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"...\"\nos.environ[\"AWS_SESSION_TOKEN\"] = \"...\"\n\n\n@Prompter(llm=\"cohere\", model_name=\"command\")\ndef me_and_mu_children(name) -&gt; int:\n    \"\"\"\n    - user: hi, my name is {name} and my children are called, aa, bb, cc, dd, ee\n    - user: |\n        how many children do I have?\n    \"\"\"\n\n\nprint(me_and_mu_children(name=\"Zud\"))\n# &gt;&gt;&gt; 5\n</code></pre></p>"},{"location":"#best-practices","title":"Best practices","text":"<p>When using Pydantic Prompter, it is recommended to explicitly specify the parameter name you wish to retrieve, as demonstrated in the example below, where title is explicitly mentioned:</p> <p><pre><code>class RecommendationTitleResponse(BaseModel):\n    title: str = Field(description=\"4 to 6 words title\")\n\n\n@Prompter(llm=\"openai\", jinja=True, model_name=\"gpt-3.5-turbo-16k\")\ndef recommendation_title(json_entries) -&gt; RecommendationTitleResponse:\n    \"\"\"\n    - user: &gt;\n        Based on the JSON entries, suggest a minimum 4 words and maximum 6 words title\n\n    - user: &gt;\n        The JSON entries:\n        {{ json_entries }}\n\n    \"\"\"\n</code></pre> Avoid the following practice where the parameter name is not explicitly stated:</p> <p><pre><code>class BaseResponse(BaseModel):\n    text: str = Field(description=\"4 to 6 words text\")\n\n\n@Prompter(llm=\"openai\", jinja=True, model_name=\"gpt-3.5-turbo-16k\")\ndef recommendation_title(json_entries) -&gt; BaseResponse:\n    \"\"\"\n    ...\n    \"\"\"\n</code></pre> Adhering to these best practices will ensure clarity and precision when using Pydantic Prompter.</p>"},{"location":"#debugging-and-logging","title":"Debugging and logging","text":"<p>You can view info and/or debugging logging using the following snippet:</p> <p><pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogging.getLogger(\"pydantic_prompter\").setLevel(logging.DEBUG)\n</code></pre> Resulting <pre><code>DEBUG:pydantic_prompter:Using OpenAI provider with model gpt-3.5-turbo\nDEBUG:pydantic_prompter:Using PydanticParser\nDEBUG:pydantic_prompter:Calling with prompt: \n {'role': 'user', 'content': 'say hi'}\nDEBUG:pydantic_prompter:Response from llm: \n {\n  \"response\": \"Hi there!\"\n }\n</code></pre></p>"},{"location":"ai_services/","title":"AI services","text":""},{"location":"ai_services/#using-aws-bedrock","title":"Using AWS Bedrock","text":"<p>If you want to use AWS Bedrock you should install bedrock-python-sdk</p> <pre><code>pip install 'pydantic-prompter[bedrock]'\n</code></pre> <p>setup your AWS creds </p> <pre><code>from pydantic_prompter import Prompter\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport os\n\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"...\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"...\"\nos.environ[\"AWS_SESSION_TOKEN\"] = \"...\"\n\n\nclass MyChildren(BaseModel):\n    num_of_children: int\n    children_names: List[str] = Field(description=\"The names of my children\")\n\n\n@Prompter(llm=\"bedrock\", model_name=\"anthropic.claude-v2\")\ndef me_and_mu_children(name) -&gt; MyChildren:\n    \"\"\"\n    - user: hi, my name is {name} and my children are called, aa, bb, cc\n    - user: |\n        how many children do I have and what's their names?\n    \"\"\"\n\n\nprint(me_and_mu_children(name=\"Ofer\"))\n</code></pre>"},{"location":"ai_services/#using-cohere","title":"Using Cohere","text":"<p>If you want to use AWS Bedrock you should install bedrock-python-sdk</p> <pre><code>pip install 'pydantic-prompter[cohere]'\n</code></pre> <p>setup your AWS creds </p> <pre><code>from pydantic_prompter import Prompter\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport os\n\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"...\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"...\"\nos.environ[\"AWS_SESSION_TOKEN\"] = \"...\"\n\n\nclass MyChildren(BaseModel):\n    num_of_children: int\n    children_names: List[str] = Field(description=\"The names of my children\")\n\n\n@Prompter(llm=\"cohere\", model_name=\"command\")\ndef me_and_mu_children(name) -&gt; MyChildren:\n    \"\"\"\n    - user: hi, my name is {name} and my children are called, aa, bb, cc, dd, ee\n    - user: |\n        how many children do I have and what's their names?\n    \"\"\"\n\n\nprint(me_and_mu_children(name=\"Zud\"))\n</code></pre>"},{"location":"prompt_templates/","title":"Prompts","text":""},{"location":"prompt_templates/#prompts","title":"Prompts","text":""},{"location":"prompt_templates/#working-with-custom-prompts","title":"Working with custom prompts","text":"<p>Relevant to Anthropic models</p> <pre><code>import logging\nimport os\nfrom pydantic_prompter import Prompter\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nlogging.basicConfig(\n    level=logging.INFO,\n)\nlogging.getLogger(\"pydantic_prompter\").setLevel(logging.DEBUG)\n\nos.environ[\"TEMPLATE_PATHS__COHERE\"] = \"./cohere_custom.jinja\"\n\n\nclass MyChildren(BaseModel):\n    num_of_children: int\n    children_names: List[str] = Field(description=\"The names of my children\")\n\n\n@Prompter(llm=\"bedrock\", model_name=\"anthropic.claude-v1\")\ndef me_and_mu_children(name) -&gt; MyChildren:\n    \"\"\"\n    - user: hi, my name is {name} and my children are called, aa, bb, cc\n    - user: |\n        how many children do I have and what's their names?\n    \"\"\"\n\n\nprint(me_and_mu_children(name=\"Ofer\"))\n\n# &gt;&gt;&gt; DEBUG:pydantic_prompter:Using bedrock provider\n#     DEBUG:pydantic_prompter:Using BedRockAnthropic provider with model anthropic.claude-v1\n#     DEBUG:pydantic_prompter:Using PydanticParser\n#     INFO:pydantic_prompter:Using custom prompt from ./anthropic_custom.jinja\n#     DEBUG:pydantic_prompter:Calling with prompt:\n#      Human: You are a REST API that answers the question contained in &lt;qq&gt; tags.\n#     Your response should be in a JSON format which it's schema is specified in the ...\n#\n#     &lt;json&gt;\n#     {\n</code></pre>"},{"location":"prompt_templates/#predefined-prompts","title":"Predefined prompts","text":""},{"location":"prompt_templates/#cohere","title":"Cohere","text":"<pre><code>System: Act like a REST API that answers the question contained in &lt;question&gt; tags.\nYour response should be within a JSON markdown block in JSON format with the schema specified in the &lt;json_schema&gt; tags.\nDO NOT add any other text other than the JSON response\n\n&lt;json_schema&gt;\n{{ schema }}\n&lt;/json_schema&gt;\n\n&lt;question&gt;\n{{ question }}\n&lt;/question&gt;\n\nChatbot: ```json\\n\n</code></pre>"},{"location":"supported_models/","title":"Supported models","text":""},{"location":"supported_models/#supported-models","title":"Supported Models","text":""},{"location":"supported_models/#openai","title":"OpenAI","text":"<ul> <li>gpt-3.5-turbo</li> <li>gpt-4</li> <li>gpt-4-turbo-preview</li> </ul>"},{"location":"supported_models/#bedrock","title":"Bedrock","text":"<ul> <li>meta.llama2-70b-chat-v1</li> <li>meta.llama2-13b-chat-v1</li> <li>anthropic.claude-instant-v1</li> <li>anthropic.claude-v1</li> <li>anthropic.claude-v2</li> <li>anthropic.claude-3-sonnet-20240229-v1:0</li> </ul>"},{"location":"supported_models/#cohere","title":"Cohere","text":"<ul> <li>command</li> <li>command-light</li> </ul>"}]}